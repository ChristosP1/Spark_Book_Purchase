{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Parameter determining if results will be visualized in cells\n",
        "VISUALIZATION = False\n",
        "\n",
        "# Dataset choice\n",
        "DATASET_NAME = \"dataset_large.txt\""
      ],
      "metadata": {
        "id": "FgLP1jo3U755"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaUdski5BxWl"
      },
      "source": [
        "# Initial steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEiemrqpBxWn"
      },
      "source": [
        "## 1.Libraries and environment initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYTp1JYjBxWo",
        "outputId": "165c9cff-b174-4e48-ffe5-5ce3e837d248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u412-ga-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.25.2)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n",
            "Requirement already satisfied: kneed in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.11.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install kneed\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nfzx14riBxWn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, Row, types as T\n",
        "from pyspark.sql.functions import col, rank, row_number, split, regexp_replace, collect_list, hash as spark_hash, concat_ws, concat, lit, when, abs, explode, desc, size\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, LongType, DoubleType, IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import MinHashLSH\n",
        "import pandas as pd\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from graphframes import GraphFrame\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain\n",
        "from pyspark.ml.feature import PCA as PCA_Spark, VectorAssembler, MinMaxScaler#, StandardScaler, RobustScaler\n",
        "from pyspark.ml.clustering import BisectingKMeans\n",
        "from kneed import KneeLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT48Bz-NBxWp"
      },
      "source": [
        "## 2.Create & Configure Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "ip1w21gRBxWp",
        "outputId": "11fc2165-4d2e-4959-f1e8-38c4abc1e4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark session created successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ba3b66d9900>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9b38c11d9553:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[16]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DIS-lab-1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def create_session():\n",
        "    # create the session\n",
        "    conf = SparkConf()\n",
        "    conf.setAppName(\"DIS-lab-1\")    # Sets name of the Spark Application\n",
        "    conf.setMaster(\"local[16]\")    # Master URL. In this case local[*] uses all the available cores in the machine\n",
        "    conf.set(\"spark.driver.memory\", \"10G\")   # Memory allocated to driver process\n",
        "    conf.set(\"spark.driver.maxResultSize\", \"6G\")    # Maximum size of results that can be returned to driver\n",
        "    conf.set(\"spark.executor.instances\", \"4\")\n",
        "    conf.set(\"spark.executor.cores\", \"4\")\n",
        "    conf.set(\"spark.executor.memory\", \"4G\")    # Memory allocated to each executor\n",
        "    conf.set(\"spark.network.timeout\", \"600s\")  # Increase network timeout\n",
        "    conf.set(\"spark.executor.heartbeatInterval\", \"60s\")  # Increase heartbeat interval\n",
        "    conf.set(\"spark.rpc.message.maxSize\", \"512\")  # Increase max message size\n",
        "    conf.set(\"spark.driver.maxResultSize\", \"4G\")  # Increase driver max result size\n",
        "    conf.set(\"spark.sql.broadcastTimeout\", \"600\")  # Increase broadcast timeout\n",
        "    conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Increase the number of shuffle partitions\n",
        "    conf.set(\"spark.yarn.executor.memoryOverhead\", \"2048\")  # Increase memory overhead\n",
        "    conf.set(\"spark.memory.offHeap.enabled\",\"true\")\n",
        "    conf.set(\"spark.memory.offHeap.size\",\"10g\")\n",
        "    conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\")  # Add GraphFrames to the spark configuration\n",
        "    conf.set(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")  # Ensure local file system is used for checkpointing\n",
        "\n",
        "    sc = pyspark.SparkContext(conf=conf)    # Initializes the Spark context with this specific configuration\n",
        "    spark = SparkSession.builder.config(conf=sc.getConf()).getOrCreate()    # Creates Spark session\n",
        "\n",
        "    # Set checkpoint directory\n",
        "    sc.setCheckpointDir(\"checkpoints\")\n",
        "\n",
        "    return sc, spark\n",
        "\n",
        "try:\n",
        "    if 'sc' in globals() and sc is not None:\n",
        "        sc.stop()\n",
        "        print(\"--Stopped existing SparkContext\")\n",
        "    if 'spark' in globals() and isinstance(spark, SparkSession):\n",
        "        spark.stop()\n",
        "        print(\"--Stopped existing SparkSession\")\n",
        "except Exception as e:\n",
        "    print(f\"Error stopping existing Spark session or context: {e}\")\n",
        "\n",
        "# Create a new Spark session\n",
        "sc, spark = create_session()\n",
        "print(\"Spark session created successfully!\")\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bEQ3ID-BxWq"
      },
      "source": [
        "## 3.Load and Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khtRtikaBxWr"
      },
      "source": [
        "### Load the data from the text file into Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r8KCxiEOBxWr"
      },
      "outputs": [],
      "source": [
        "logs_txt = spark.sparkContext.textFile(DATASET_NAME)\n",
        "logs = logs_txt.map(lambda x: (x, )).toDF([\"Logs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6UDhMxuBxWs"
      },
      "source": [
        "### Split the data into 5 separate columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KSrihLbHBxWs"
      },
      "outputs": [],
      "source": [
        "logs_splitted = logs \\\n",
        "    .withColumn(\"from_server\", regexp_replace(split(col(\"Logs\"), \", \").getItem(0), \"[<>]\", \"\")) \\\n",
        "    .withColumn(\"to_server\", split(col(\"Logs\"), \", \").getItem(1)) \\\n",
        "    .withColumn(\"time\", split(col(\"Logs\"), \", \").getItem(2)) \\\n",
        "    .withColumn(\"action\", split(col(\"Logs\"), \", \").getItem(3)) \\\n",
        "    .withColumn(\"process_id\", regexp_replace(split(col(\"Logs\"), \", \").getItem(4), \"[<>]\", \"\")) \\\n",
        "    .drop(\"Logs\")\n",
        "\n",
        "# Cast the \"time\" and \"process_id\" columns to integers\n",
        "logs_casted = logs_splitted \\\n",
        "    .withColumn(\"time\", col(\"time\").cast(\"integer\")) \\\n",
        "    .withColumn(\"process_id\", col(\"process_id\").cast(\"integer\"))\n",
        "\n",
        "if VISUALIZATION:\n",
        "  logs_splitted.show(1, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CYhr81mBxWs"
      },
      "source": [
        "## 4.Prepare data for processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wmZXjbJBxWs"
      },
      "source": [
        "### Group by process_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rFPSRoCDBxWt"
      },
      "outputs": [],
      "source": [
        "logs_grouped = logs_casted.groupBy(\"process_id\").agg(\n",
        "    collect_list(\"from_server\").alias(\"from_servers\"),\n",
        "    collect_list(\"to_server\").alias(\"to_servers\"),\n",
        "    collect_list(\"time\").alias(\"times\"),\n",
        "    collect_list(\"action\").alias(\"actions\")\n",
        ")\n",
        "logs_grouped_not_sorted = logs_grouped\n",
        "\n",
        "if VISUALIZATION:\n",
        "  logs_grouped.show(6, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8SiZdCrBxWt"
      },
      "source": [
        "### Sort all the column items based on 'from_server' so the sub-processes are in the right order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AJaqqqzSBxWt"
      },
      "outputs": [],
      "source": [
        "def sort_lists_time(times, from_servers, to_servers, actions):\n",
        "    combined = list(zip(times, from_servers, to_servers, actions))\n",
        "    sorted_combined = sorted(combined, key=lambda x: x[0])\n",
        "\n",
        "    # Remove the second half of each list since it contains duplicte names\n",
        "    half_length = len(sorted_combined) // 2\n",
        "    truncated_combined = sorted_combined[:half_length]\n",
        "\n",
        "    times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted = zip(*truncated_combined)\n",
        "    return list(times_sorted), list(from_servers_sorted), list(to_servers_sorted), list(actions_sorted)\n",
        "\n",
        "def sort_lists_from_server(times, from_servers, to_servers, actions):\n",
        "    # First sort by time\n",
        "    combined = list(zip(times, from_servers, to_servers, actions))\n",
        "    sorted_combined = sorted(combined, key=lambda x: x[0])  # Sort by times (x[0])\n",
        "\n",
        "    # Remove the second half of each list since it contains duplicte names\n",
        "    half_length = len(sorted_combined) // 2\n",
        "    truncated_combined = sorted_combined[:half_length]\n",
        "\n",
        "    times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted = zip(*truncated_combined)\n",
        "\n",
        "    combined = list(zip(times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted))\n",
        "    sorted_combined = sorted(combined, key=lambda x: x[1])\n",
        "    times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted = zip(*sorted_combined)\n",
        "    return list(times_sorted), list(from_servers_sorted), list(to_servers_sorted), list(actions_sorted)\n",
        "\n",
        "# Define the schema for the sorted columns\n",
        "sorted_lists_schema = StructType([\n",
        "    StructField(\"times\", ArrayType(LongType()), nullable=True),\n",
        "    StructField(\"from_servers\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"to_servers\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), nullable=True)\n",
        "])\n",
        "\n",
        "# Register the function as a UDF\n",
        "sort_lists_udf = udf(sort_lists_from_server, sorted_lists_schema)\n",
        "\n",
        "# Apply the UDF to sort the lists based on the \"times\" column\n",
        "logs_grouped = logs_grouped.withColumn(\"sorted_lists\", sort_lists_udf(\"times\", \"from_servers\", \"to_servers\", \"actions\"))\n",
        "\n",
        "# Split the sorted lists into separate columns\n",
        "logs_grouped = logs_grouped.withColumn(\"times\", col(\"sorted_lists.times\")) \\\n",
        "                           .withColumn(\"from_servers\", col(\"sorted_lists.from_servers\")) \\\n",
        "                           .withColumn(\"to_servers\", col(\"sorted_lists.to_servers\")) \\\n",
        "                           .withColumn(\"actions\", col(\"sorted_lists.actions\")) \\\n",
        "                           .drop(\"sorted_lists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SylZDcJZBxWt"
      },
      "source": [
        "### Create 'process_string' column with a string of all the 'from_servers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oTNqHom-BxWt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Using the withColumn method to create a new column \"process_string\".\n",
        "The concat_ws function concatenates multiple column values into a single string, separated by a comma.\n",
        "col(\"from_servers\"): Selects the column \"from_servers\".\n",
        "col(\"to_servers\"): Selects the column \"to_servers\".\n",
        "col(\"times\"): Selects the column \"times\".\n",
        "col(\"actions\"): Selects the column \"actions\".\n",
        "The concatenated string is stored in the new column \"process_string\".\n",
        "'''\n",
        "\n",
        "logs_grouped = logs_grouped.withColumn(\n",
        "    \"process_string\",\n",
        "    concat_ws(\",\", col(\"from_servers\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCPqIDm1BxWu"
      },
      "source": [
        "# Part 1: \"Cleaning\": LSH and Connected Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCcyD7F3BxWu"
      },
      "source": [
        "## Step 1. Create shingles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4wKnb0yBxWu"
      },
      "source": [
        "### 1.1. Pre-processing before shingles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ZhuZlQBxWu"
      },
      "source": [
        "#### Find common sub-string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W2r6dsB-BxWu"
      },
      "outputs": [],
      "source": [
        "def get_all_substrings(string, min_length):\n",
        "    length = len(string)\n",
        "    return [string[i:j] for i in range(length) for j in range(i + min_length, length + 1)]\n",
        "\n",
        "\n",
        "def find_common_substring(server_names, min_length=3, min_occurrences_ratio=0.6):\n",
        "    substr_counter = Counter()\n",
        "    total_names = len(server_names)\n",
        "    min_occurrences = int(total_names * min_occurrences_ratio)\n",
        "\n",
        "    for name in server_names:\n",
        "        substrings = get_all_substrings(name, min_length)\n",
        "        substr_counter.update(substrings)\n",
        "\n",
        "    # Filter substrings that occur at least min_occurrences times\n",
        "    common_substrings = [substr for substr, count in substr_counter.items() if count >= min_occurrences]\n",
        "\n",
        "    if not common_substrings:\n",
        "        return \"\"\n",
        "\n",
        "    # Return the longest common substring\n",
        "    return max(common_substrings, key=len)\n",
        "\n",
        "\n",
        "def remove_common_substring(strings, common_substring):\n",
        "    return [string.replace(common_substring, \"\") for string in strings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lC5ysGS9BxWu"
      },
      "outputs": [],
      "source": [
        "# Sample 5 rows from the data\n",
        "sample_rows = logs_grouped.select(\"from_servers\").take(3)\n",
        "\n",
        "# Extract the \"from_servers\" lists from the sample rows\n",
        "sample_from_servers = [row[\"from_servers\"] for row in sample_rows]\n",
        "\n",
        "# Flatten the list of lists to a single list of server names\n",
        "flattened_server_names = [server for sublist in sample_from_servers for server in sublist]\n",
        "\n",
        "# Get unique server names\n",
        "unique_server_names = list(set(flattened_server_names))\n",
        "\n",
        "# Use the function on the unique server names\n",
        "common_substring = find_common_substring(unique_server_names, min_length=3, min_occurrences_ratio=0.70)\n",
        "\n",
        "if VISUALIZATION:\n",
        "  print(\"Most Common Substring:\", common_substring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q464I4cGBxWv"
      },
      "source": [
        "#### Remove common sub-string + special characters + spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oC5bXXTwBxWv"
      },
      "outputs": [],
      "source": [
        "# Create a new DataFrame with the common substring removed from the process_string\n",
        "logs_grouped_cleaned = logs_grouped.withColumn(\n",
        "    \"process_string_clean_common\",\n",
        "    regexp_replace(col(\"process_string\"), common_substring, \"\"),\n",
        ")\n",
        "\n",
        "logs_grouped_cleaned = logs_grouped_cleaned.withColumn(\n",
        "    \"process_string_cleaned\",\n",
        "    regexp_replace(col(\"process_string_clean_common\"), \"[^a-zA-Z0-9\\s]\", \"\")\n",
        ")\n",
        "\n",
        "# Show the cleaned DataFrame\n",
        "if VISUALIZATION:\n",
        "  logs_grouped_cleaned.select(\"process_string_cleaned\").limit(1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEAV8pGFBxWv"
      },
      "source": [
        "### 1.2. Define the shingles of each function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rXrJBNF2BxWv"
      },
      "outputs": [],
      "source": [
        "# Define the shingling function for process strings\n",
        "def shingle_process_string(process_string, k):\n",
        "    shingle_set = set()\n",
        "    for i in range(len(process_string) - k + 1):\n",
        "        shingle = process_string[i:i + k]\n",
        "        shingle_set.add(shingle)\n",
        "    return list(shingle_set)\n",
        "\n",
        "# Define the shingle length based on the average length of the servers of the first 5 rows\n",
        "first_rows = logs_grouped_cleaned.take(5)\n",
        "average_length = 0\n",
        "\n",
        "# Calculate the average length of server names\n",
        "total_length = 0\n",
        "total_servers = 0\n",
        "\n",
        "# For each row in the sample rows sum the length of the names and the number of servers\n",
        "for row in first_rows:\n",
        "    servers = row[\"process_string_clean_common\"]\n",
        "    server_names = servers.split(',')\n",
        "    total_length += sum([len(server) for server in server_names])\n",
        "    total_servers += len(server_names)\n",
        "\n",
        "# Find the average server name length\n",
        "average_server_name_length = total_length / total_servers\n",
        "\n",
        "# Devine ths shingle length as the 30% of the average server name (or 2 if the names are too short)\n",
        "shingle_length_percentage = 0.3\n",
        "shingle_length = max(2, int(np.ceil(average_server_name_length *shingle_length_percentage)))\n",
        "\n",
        "if VISUALIZATION:\n",
        "  print(f'Average length of server names: {average_server_name_length}')\n",
        "  print(f'Determined shingle length: {shingle_length}')\n",
        "\n",
        "# Register the UDF for shingling\n",
        "shingle_udf = udf(lambda process_string: shingle_process_string(process_string, shingle_length), ArrayType(StringType()))\n",
        "\n",
        "# Apply the UDF to create shingles from process strings\n",
        "logs_grouped_cleaned_with_shingles = logs_grouped_cleaned.withColumn(\"shingles\", shingle_udf(col(\"process_string_cleaned\")))\n",
        "\n",
        "# Show the resulting DataFrame with shingles\n",
        "if VISUALIZATION:\n",
        "  logs_grouped_cleaned_with_shingles.select(\"process_id\", \"process_string_cleaned\", \"shingles\").limit(1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQUcAWsqBxWw"
      },
      "source": [
        "### 1.3. Create Vocabulary and Sparse Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44SUu8ZyBxWw"
      },
      "source": [
        "#### Keep all unique shingles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3T3YnNwGBxWw"
      },
      "outputs": [],
      "source": [
        "# Collect all shingles from the DataFrame into a single list\n",
        "all_unique_shingles = logs_grouped_cleaned_with_shingles.select(\"shingles\").rdd.flatMap(lambda row: row.shingles).distinct().collect()\n",
        "\n",
        "# Create a vocabulary dictionary where each shingle is assigned a unique index\n",
        "vocab = {shingle: idx for idx, shingle in enumerate(all_unique_shingles)}\n",
        "# Print the vocabulary to verify\n",
        "if VISUALIZATION:\n",
        "  print(f'Vocabulary: {str(list(vocab)[:10])[:-1]} . . . ]')\n",
        "  print(f'Length: {len(vocab)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0I21nVjBxWw"
      },
      "source": [
        "#### Create the sparse vector of shingle occurence for each process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2GHSY2xWBxWw"
      },
      "outputs": [],
      "source": [
        "# Define a function to create sparse vectors based on the vocabulary\n",
        "def create_sparse_vector_for_lsh(shingles, vocab):\n",
        "    indices = []\n",
        "    for shingle in shingles:\n",
        "        if shingle in vocab:\n",
        "            idx = vocab[shingle]\n",
        "            indices.append(idx)\n",
        "\n",
        "    indices = sorted(indices)  # Sort indices\n",
        "    values = [1.0] * len(indices)\n",
        "    return Vectors.sparse(len(vocab), indices, values)\n",
        "\n",
        "# Register the UDF to create sparse vectors for LSH\n",
        "sparse_vector_for_lsh_udf = udf(lambda shingles: create_sparse_vector_for_lsh(shingles, vocab), VectorUDT())\n",
        "\n",
        "# Apply the UDF to create sparse vectors directly suitable for MinHashLSH\n",
        "logs_sparse_vectors = logs_grouped_cleaned_with_shingles.withColumn(\"features\", sparse_vector_for_lsh_udf(col(\"shingles\")))\n",
        "\n",
        "# Show the resulting DataFrame with sparse vectors\n",
        "if VISUALIZATION:\n",
        "  logs_sparse_vectors.select(\"process_id\", \"features\").limit(1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yE937uXBxWx"
      },
      "source": [
        "## Step 2. MinHash and LSH with pyspark's MinHashLSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBhmKuIJBxWx"
      },
      "source": [
        "### 2.1. Create MinHashLSH model and fit it with the 'features' column of the 'logs_sparse_vectors' dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gVfgolfSBxWx"
      },
      "outputs": [],
      "source": [
        "# Initialize MinHashLSH\n",
        "'''\n",
        ":inputCol: The column of our dataframe that is going to be used for hashing\n",
        ":outputCol: The new solumn that will be created that includes the hashes\n",
        ":numHashTables: Number of MinHash functions\n",
        ":seed: Seed for reproducibility\n",
        "\n",
        "'''\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=30, seed=12345)\n",
        "\n",
        "# Fit the model\n",
        "model = mh.fit(logs_sparse_vectors)\n",
        "\n",
        "# Transform the data to include the hashes\n",
        "transformed_df = model.transform(logs_sparse_vectors)\n",
        "\n",
        "# Show the resulting DataFrame with hashes\n",
        "if VISUALIZATION:\n",
        "  transformed_df.select(\"process_id\", \"hashes\").limit(1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlRniUKaBxWx"
      },
      "source": [
        "### 2.2. Find pairs with Jaccard distance < threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "46Uj0cQzBxWx"
      },
      "outputs": [],
      "source": [
        "# Find similar pairs with Jaccard similarity above a threshold\n",
        "max_distance_threshold = 0.425\n",
        "similar_pairs_df = model.approxSimilarityJoin(transformed_df, transformed_df, max_distance_threshold, distCol=\"JaccardDistance\") \\\n",
        "    .select(col(\"datasetA.process_id\").alias(\"process_id_1\"),\n",
        "            col(\"datasetB.process_id\").alias(\"process_id_2\"),\n",
        "            col(\"JaccardDistance\"))\n",
        "\n",
        "# Filter out rows where process_id_1 is equal to process_id_2\n",
        "filtered_pairs_df = similar_pairs_df.filter(col(\"process_id_1\") != col(\"process_id_2\"))\n",
        "\n",
        "if VISUALIZATION:\n",
        "  filtered_pairs_df.select(\"JaccardDistance\").describe().show()\n",
        "  filtered_pairs_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Group similar processes with connected components in graph and update dataset"
      ],
      "metadata": {
        "id": "paIsZEippLTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Construct graph using similarities between nodes as edges and find connected components"
      ],
      "metadata": {
        "id": "XpDb_PWen4Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create GraphFrame\n",
        "edges = filtered_pairs_df.select(col(\"process_id_1\").alias(\"src\"),\n",
        "                                col(\"process_id_2\").alias(\"dst\"),\n",
        "                                col(\"JaccardDistance\").alias(\"weight\"))\n",
        "vertices = edges.selectExpr(\"src as id\").union(edges.selectExpr(\"dst as id\")).distinct()\n",
        "graph = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find Connected Components\n",
        "connected_components = graph.connectedComponents()"
      ],
      "metadata": {
        "id": "z7yy7xiesmMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02aa3913-a040-44d2-99a6-9bdceaead78b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct NetworkX graph\n",
        "if VISUALIZATION:\n",
        "  nx_graph = nx.Graph()\n",
        "  for row in graph.edges.collect():\n",
        "      nx_graph.add_edge(row['src'], row['dst'])\n",
        "\n",
        "  # Visualize using matplotlib\n",
        "  pos = nx.spring_layout(nx_graph)\n",
        "  nx.draw(nx_graph, pos, with_labels=True, node_color='skyblue', node_size=500,\n",
        "          edge_color='#909090', width=0.5, font_size=8, font_color='#7c7c7c',\n",
        "          font_weight='bold')\n",
        "  plt.show()\n",
        "\n",
        "  print()\n",
        "\n",
        "  pos = nx.fruchterman_reingold_layout(nx_graph)\n",
        "  nx.draw(nx_graph, pos, with_labels=True, node_color='skyblue', node_size=500,\n",
        "          edge_color='#909090', width=0.5, font_size=8, font_color='#7c7c7c',\n",
        "          font_weight='bold')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EjnBIBCsx_yc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Keep only one out of each group of similar processes and create the final filtered dateset"
      ],
      "metadata": {
        "id": "OwmZdQiPnjok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the window specification for collecting members\n",
        "window_spec_collect = Window.partitionBy(\"component\")\n",
        "\n",
        "# Define the window specification for ordering by id\n",
        "window_spec_representative = Window.partitionBy(\"component\").orderBy(\"id\")\n",
        "\n",
        "# Use row_number to get the representative\n",
        "representatives_with_row_num = connected_components \\\n",
        "    .withColumn(\"row_num\", row_number().over(window_spec_representative))\n",
        "\n",
        "# Collect all ids of the connected components into a list, excluding the representative\n",
        "connected_components_with_list = representatives_with_row_num \\\n",
        "    .withColumn(\"members\", collect_list(\n",
        "        when(col(\"row_num\") != 1, col(\"id\"))\n",
        "    ).over(window_spec_collect))\n",
        "\n",
        "# Filter for the representative and select the required columns\n",
        "groups_with_members = connected_components_with_list \\\n",
        "    .where(col(\"row_num\") == 1) \\\n",
        "    .select(\"id\", \"members\")\n",
        "\n",
        "if VISUALIZATION:\n",
        "  groups_with_members.show()"
      ],
      "metadata": {
        "id": "9VsWiEgyt69g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all the member lists into a single list\n",
        "member_lists = groups_with_members.select(\"members\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "ids_to_drop = list(chain.from_iterable(member_lists))\n",
        "\n",
        "if VISUALIZATION:\n",
        "  print(ids_to_drop)"
      ],
      "metadata": {
        "id": "-uRAdqCskcVL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete grouped processes\n",
        "filtered_df = logs_grouped_not_sorted.filter(~col(\"process_id\").isin(ids_to_drop))\n",
        "\n",
        "filtered_df_strings = filtered_df.select(\n",
        "    col(\"process_id\"),\n",
        "    concat(lit(\"[\"), concat_ws(\",\", col(\"from_servers\")), lit(\"]\")).alias(\"from_servers\"),\n",
        "    concat(lit(\"[\"), concat_ws(\",\", col(\"to_servers\")), lit(\"]\")).alias(\"to_servers\"),\n",
        "    concat(lit(\"[\"), concat_ws(\",\", col(\"times\")), lit(\"]\")).alias(\"times\"),\n",
        "    concat(lit(\"[\"), concat_ws(\",\", col(\"actions\")), lit(\"]\")).alias(\"actions\")\n",
        ")\n",
        "\n",
        "if VISUALIZATION:\n",
        "  filtered_df_strings.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "tYAcSYqJXF0t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Export part1Output.txt & part1Observations.txt"
      ],
      "metadata": {
        "id": "SE2gORKRCACr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"part1Observations.txt\", \"w\") as f:\n",
        "    grouped = []\n",
        "    for row in groups_with_members.collect():\n",
        "        representative_id = row[\"id\"]\n",
        "        member_ids = row[\"members\"]\n",
        "        group_ids = [representative_id] + member_ids\n",
        "        grouped = grouped + group_ids\n",
        "\n",
        "        f.write(f\"Group: {{{', '.join(map(str, group_ids))}}}\\n\")\n",
        "        for process_id in group_ids:\n",
        "            f.write(f\"{process_id}:\\n\")\n",
        "            # Filter logs directly for the current process_id\n",
        "            process_logs = logs_grouped_not_sorted.filter(col(\"process_id\") == process_id).collect()\n",
        "            for log in process_logs:\n",
        "                from_servers = log['from_servers']\n",
        "                to_servers = log['to_servers']\n",
        "                times = log['times']\n",
        "                actions = log['actions']\n",
        "                process_id_val = log['process_id']\n",
        "\n",
        "                # Iterate through the lists and create log entries\n",
        "                for from_server, to_server, time, action in zip(from_servers, to_servers, times, actions):\n",
        "                    log_str = f\"<{from_server}, {to_server}, {time}, {action}, {process_id_val}>\"\n",
        "                    f.write(f\"\\t\\t\\t{log_str}\\n\")\n",
        "\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    ungrouped_df = logs_grouped_not_sorted.filter(~col(\"process_id\").isin(grouped))\n",
        "    for row in ungrouped_df.collect():\n",
        "        process_id_val = row['process_id']\n",
        "        process_logs = logs_grouped_not_sorted.filter(col(\"process_id\") == process_id_val).collect()\n",
        "        for log in process_logs:\n",
        "          from_servers = log['from_servers']\n",
        "          to_servers = log['to_servers']\n",
        "          times = log['times']\n",
        "          actions = log['actions']\n",
        "\n",
        "          f.write(f\"Group: {{{process_id_val}}}\\n\")\n",
        "          f.write(f\"{process_id_val}:\\n\")\n",
        "          # Iterate through the lists and create log entries\n",
        "          for from_server, to_server, time, action in zip(from_servers, to_servers, times, actions):\n",
        "              log_str = f\"<{from_server}, {to_server}, {time}, {action}, {process_id_val}>\"\n",
        "              f.write(f\"\\t\\t\\t{log_str}\\n\")"
      ],
      "metadata": {
        "id": "avDBNm-j6Hh6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"part1Output.txt\", \"w\") as f:\n",
        "    for row in filtered_df.collect():\n",
        "        process_id_val = row['process_id']\n",
        "        process_logs = logs_grouped_not_sorted.filter(col(\"process_id\") == process_id_val).collect()\n",
        "        for log in process_logs:\n",
        "          from_servers = log['from_servers']\n",
        "          to_servers = log['to_servers']\n",
        "          times = log['times']\n",
        "          actions = log['actions']\n",
        "\n",
        "          # Iterate through the lists and create log entries\n",
        "          for from_server, to_server, time, action in zip(from_servers, to_servers, times, actions):\n",
        "              log_str = f\"<{from_server}, {to_server}, {time}, {action}, {process_id_val}>\"\n",
        "              f.write(f\"{log_str}\\n\")"
      ],
      "metadata": {
        "id": "trHfgJ_h9wKK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The original dataset had: {logs_grouped.count()} processes and we kept: {filtered_df.count()}')\n",
        "print(f'Groups created: {groups_with_members.count()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jcbq0QPX1BV",
        "outputId": "b65479ef-c1c5-46e4-8411-d80b2960dc18"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original dataset had: 1000 processes and we kept: 221\n",
            "Groups created: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: \"Categorization\""
      ],
      "metadata": {
        "id": "gaHT9hOSCi1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import & format data from Part 1"
      ],
      "metadata": {
        "id": "cdkE0AirDYdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logs_txt = spark.sparkContext.textFile(\"part1Output.txt\")\n",
        "logs = logs_txt.map(lambda x: (x, )).toDF([\"Logs\"])\n",
        "\n",
        "logs_splitted = logs \\\n",
        "    .withColumn(\"from_server\", regexp_replace(split(col(\"Logs\"), \", \").getItem(0), \"[<>]\", \"\")) \\\n",
        "    .withColumn(\"to_server\", split(col(\"Logs\"), \", \").getItem(1)) \\\n",
        "    .withColumn(\"time\", split(col(\"Logs\"), \", \").getItem(2)) \\\n",
        "    .withColumn(\"action\", split(col(\"Logs\"), \", \").getItem(3)) \\\n",
        "    .withColumn(\"process_id\", regexp_replace(split(col(\"Logs\"), \", \").getItem(4), \"[<>]\", \"\")) \\\n",
        "    .drop(\"Logs\")\n",
        "\n",
        "# Cast the \"time\" and \"process_id\" columns to integers\n",
        "logs_casted = logs_splitted \\\n",
        "    .withColumn(\"time\", col(\"time\").cast(\"integer\")) \\\n",
        "    .withColumn(\"process_id\", col(\"process_id\").cast(\"integer\"))\n",
        "\n",
        "# Group\n",
        "logs_grouped = logs_casted.groupBy(\"process_id\").agg(\n",
        "    collect_list(\"from_server\").alias(\"from_servers\"),\n",
        "    collect_list(\"to_server\").alias(\"to_servers\"),\n",
        "    collect_list(\"time\").alias(\"times\"),\n",
        "    collect_list(\"action\").alias(\"actions\")\n",
        ")\n",
        "logs_grouped_not_sorted = logs_grouped\n",
        "\n",
        "# Define the schema for the sorted columns\n",
        "sorted_lists_schema = StructType([\n",
        "    StructField(\"times\", ArrayType(LongType()), nullable=True),\n",
        "    StructField(\"from_servers\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"to_servers\", ArrayType(StringType()), nullable=True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), nullable=True)\n",
        "])\n",
        "\n",
        "# Register the function as a UDF\n",
        "sort_lists_udf = udf(sort_lists_from_server, sorted_lists_schema)\n",
        "\n",
        "# Apply the UDF to sort the lists based on the \"times\" column\n",
        "logs_grouped = logs_grouped.withColumn(\"sorted_lists\", sort_lists_udf(\"times\", \"from_servers\", \"to_servers\", \"actions\"))\n",
        "\n",
        "# Split the sorted lists into separate columns\n",
        "logs_grouped = logs_grouped.withColumn(\"times\", col(\"sorted_lists.times\")) \\\n",
        "                           .withColumn(\"from_servers\", col(\"sorted_lists.from_servers\")) \\\n",
        "                           .withColumn(\"to_servers\", col(\"sorted_lists.to_servers\")) \\\n",
        "                           .withColumn(\"actions\", col(\"sorted_lists.actions\")) \\\n",
        "                           .drop(\"sorted_lists\")\n",
        "\n",
        "logs = logs_grouped.dropDuplicates(['process_id'])\n",
        "logs = logs_grouped.drop(*['to_servers', 'actions'])\n",
        "\n",
        "if VISUALIZATION:\n",
        "  logs.limit(5).show(truncate=False)"
      ],
      "metadata": {
        "id": "_CiqTon6Cu72"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Create features for clustering"
      ],
      "metadata": {
        "id": "K1C_oSeDDveN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Create total and average time taken features"
      ],
      "metadata": {
        "id": "q9AQYlQLZMV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total time taken\n",
        "logs = logs.withColumn('sum_times', F.expr(\"aggregate(times, CAST(0 AS bigint), (acc, x) -> acc + x)\"))\n",
        "\n",
        "# Get average time taken for each server\n",
        "logs = logs.withColumn('avg_times', F.expr(\"sum_times / size(times)\"))\n",
        "\n",
        "logs = logs.drop(\"times\")\n",
        "\n",
        "if VISUALIZATION:\n",
        "  logs.limit(5).show(truncate=False)"
      ],
      "metadata": {
        "id": "_Bp2Esb2Cu2B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Create server hops and base server names as one hot encoding features"
      ],
      "metadata": {
        "id": "_RzTGliAZaeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get server hops\n",
        "logs = logs.withColumn('server_hops', F.size(F.col('from_servers')))\n",
        "\n",
        "\"\"\"\n",
        "# Get unique server names\n",
        "unique_servers = logs.select(F.explode('from_servers').alias('server')).distinct()\n",
        "\n",
        "# Pivot to create one-hot encoded columns for servers\n",
        "logs = logs.join(unique_servers, F.array_contains(logs['from_servers'], unique_servers['server']), 'left_outer') \\\n",
        "           .groupBy(logs.columns) \\\n",
        "           .pivot('server') \\\n",
        "           .agg(F.lit(1).alias('dummy')) \\\n",
        "           .na.fill(0)\n",
        "\"\"\"\n",
        "\n",
        "# Remove numbers to get base server names, filter out base servers that are also in from_servers\n",
        "logs = logs.withColumn('base_servers', F.expr(\"transform(from_servers, x -> regexp_replace(x, '[0-9]+', ''))\"))\n",
        "logs = logs.withColumn('base_servers', F.array_except('base_servers', 'from_servers'))\n",
        "logs = logs.drop('from_servers')\n",
        "\n",
        "# Get unique base server names\n",
        "unique_base_servers = logs.select(F.explode('base_servers').alias('base_server')).distinct()\n",
        "\n",
        "# Pivot to create one-hot encoded columns for base servers\n",
        "logs = logs.join(unique_base_servers, F.array_contains(logs['base_servers'], unique_base_servers['base_server']), 'left_outer') \\\n",
        "           .groupBy(logs.columns) \\\n",
        "           .pivot('base_server') \\\n",
        "           .agg(F.lit(1).alias('dummy')) \\\n",
        "           .na.fill(0)\n",
        "\n",
        "logs = logs.drop('base_servers')\n",
        "\n",
        "if VISUALIZATION:\n",
        "  logs.limit(5).show(truncate=False)"
      ],
      "metadata": {
        "id": "bi0csSDhCuzv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Scale features that aren't one hot encoded and vectorize"
      ],
      "metadata": {
        "id": "7II3nJ2PZ2pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features\n",
        "assembler = VectorAssembler(inputCols=['sum_times', 'avg_times', 'server_hops'], outputCol='features_to_scale')\n",
        "feature_logs = assembler.transform(logs)\n",
        "scaler = MinMaxScaler(inputCol='features_to_scale', outputCol='scaled_features')\n",
        "scaler_model = scaler.fit(feature_logs)\n",
        "feature_logs = scaler_model.transform(feature_logs)\n",
        "\"\"\"\n",
        "scaler = StandardScaler(inputCol='features_to_scale', outputCol='scaled_features', withMean=True, withStd=True)\n",
        "scaler_model = scaler.fit(feature_logs)\n",
        "feature_logs = scaler_model.transform(feature_logs)\n",
        "scaler = RobustScaler(inputCol='features_to_scale', outputCol='scaled_features')\n",
        "scaler_model = scaler.fit(feature_logs)\n",
        "feature_logs = scaler_model.transform(feature_logs)\n",
        "\"\"\"\n",
        "feature_logs = feature_logs.drop(*['sum_times', 'avg_times', 'server_hops', 'features_to_scale'])\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[col for col in feature_logs.columns if col != 'process_id'], outputCol='features')\n",
        "feature_logs = assembler.transform(feature_logs)\n",
        "\n",
        "if VISUALIZATION:\n",
        "  feature_logs.limit(5).show(truncate=False)"
      ],
      "metadata": {
        "id": "N1b3xA4FDg55"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Reduce features to 2D using PCA"
      ],
      "metadata": {
        "id": "bpwvleAYwU2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  pca = PCA_Spark(k=2, inputCol='features', outputCol='pca_features')\n",
        "  pca_model = pca.fit(feature_logs)\n",
        "  feature_logs = pca_model.transform(feature_logs)\n",
        "\n",
        "  if VISUALIZATION:\n",
        "    feature_logs.limit(5).show(truncate=False)"
      ],
      "metadata": {
        "id": "fLyyDOMJuUo5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Cluster using Bisecting K-means"
      ],
      "metadata": {
        "id": "4Gvhk_KvbZHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Cluster features several times with batches to find optimal k with Elbow method"
      ],
      "metadata": {
        "id": "mwfD9kn-aAFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of k values to test\n",
        "batch_size = 10\n",
        "min_k = 2\n",
        "max_k = logs.count()\n",
        "# Ensure batch size of k for correct elbow method results\n",
        "k_values = list(range(min_k, max(max_k + 1, min_k + batch_size)))\n",
        "print(f'Search for k: {k_values}')\n",
        "\n",
        "# Function to run the clustering in batches\n",
        "def run_batches(k_values, batch_size, features='features'):\n",
        "    wssse_values = []\n",
        "    previous_optimal_k = None\n",
        "    while k_values:\n",
        "        batch_k = k_values[:batch_size]\n",
        "        k_values = k_values[batch_size:]\n",
        "\n",
        "        for k in batch_k:\n",
        "            kmeans = BisectingKMeans(k=k, seed=50, featuresCol=features)\n",
        "            model = kmeans.fit(feature_logs)\n",
        "            wssse = model.summary.trainingCost\n",
        "            wssse_values.append((k, wssse))\n",
        "\n",
        "        # Extract K and WSSSE values\n",
        "        k_vals, wssse_vals = zip(*wssse_values)\n",
        "\n",
        "        # Use the Kneedle algorithm to find the elbow point\n",
        "        kneedle = KneeLocator(k_vals, wssse_vals, curve='convex', direction='decreasing')\n",
        "        optimal_k = kneedle.knee\n",
        "\n",
        "        if optimal_k == previous_optimal_k:\n",
        "            break\n",
        "        previous_optimal_k = optimal_k\n",
        "\n",
        "    return optimal_k, k_vals, wssse_vals\n",
        "\n",
        "# Run the clustering in batches\n",
        "optimal_k_nonpca, k_vals_nonpca, wssse_vals_nonpca = run_batches(k_values, batch_size)\n",
        "print(f'The optimal number of clusters with normal features is: {optimal_k_nonpca}')\n",
        "\n",
        "if VISUALIZATION:\n",
        "  # Plot the WSSSE values against the k values\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(k_vals_nonpca, wssse_vals_nonpca, 'bx-')\n",
        "  plt.xlabel('Number of clusters (k)')\n",
        "  plt.xticks(k_vals_nonpca)\n",
        "  plt.ylabel('WSSSE')\n",
        "  plt.title('Elbow Method For Optimal k')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "xDG50SVQDg4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f34dc8-34be-4c34-d3e8-e86f7297dcfd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search for k: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]\n",
            "The optimal number of clusters with normal features is: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Cluster PCA features several times with batches to find optimal k with Elbow method"
      ],
      "metadata": {
        "id": "WLkdpv4KyIIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure batch size of k for correct elbow method results\n",
        "k_values = list(range(min_k, max(max_k + 1, min_k + batch_size)))\n",
        "print(f'Search for k: {k_values}')\n",
        "\n",
        "# Run the clustering in batches\n",
        "optimal_k_pca, k_vals_pca, wssse_vals_pca = run_batches(k_values, batch_size, features=\"pca_features\")\n",
        "print(f'The optimal number of clusters with 2D PCA features is: {optimal_k_pca}')\n",
        "\n",
        "if VISUALIZATION:\n",
        "  # Plot the WSSSE values against the k values\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(k_vals_pca, wssse_vals_pca, 'bx-')\n",
        "  plt.xlabel('Number of clusters (k)')\n",
        "  plt.xticks(k_vals_pca)\n",
        "  plt.ylabel('WSSSE')\n",
        "  plt.title('Elbow Method For Optimal k with 2D PCA features')\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJj7ZLERwtiS",
        "outputId": "4fb2d109-a952-40db-e8b0-4a85a3229167"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search for k: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221]\n",
            "The optimal number of clusters with 2D PCA features is: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Choose optimal k and best features"
      ],
      "metadata": {
        "id": "N8hUtjMpySS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller k and less error preferred\n",
        "k_idx_nonpca = k_vals_nonpca.index(optimal_k_nonpca)\n",
        "k_idx_pca = k_vals_pca.index(optimal_k_pca)\n",
        "if optimal_k_nonpca <= optimal_k_pca and wssse_vals_nonpca[k_idx_nonpca] <= wssse_vals_pca[k_idx_pca]:\n",
        "  optimal_k = optimal_k_nonpca\n",
        "  features = 'features'\n",
        "  print(f'The optimal number of clusters is {optimal_k_nonpca} and uses normal features')\n",
        "else:\n",
        "  optimal_k = optimal_k_pca\n",
        "  features = 'pca_features'\n",
        "  print(f'The optimal number of clusters is {optimal_k_pca} and uses 2D PCA features')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mANwo_jizE08",
        "outputId": "ea7c51f3-b841-4bdf-d669-0a98829c93e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal number of clusters is 7 and uses 2D PCA features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Cluster with optimal k and best features"
      ],
      "metadata": {
        "id": "_q3uke22bu9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final model using the optimal k\n",
        "kmeans = BisectingKMeans(k=optimal_k, seed=123, featuresCol=features)\n",
        "model = kmeans.fit(feature_logs)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(feature_logs)\n",
        "\n",
        "# Show cluster centers\n",
        "\"\"\"\n",
        "print(\"Cluster Centers:\")\n",
        "centers = model.clusterCenters()\n",
        "for idx, center in enumerate(centers):\n",
        "    print(f\"Cluster {idx}: {center}\")\n",
        "\"\"\"\n",
        "\n",
        "# Group predictions\n",
        "grouped_predictions = predictions.groupBy('prediction').agg(collect_list('process_id').alias('process_ids')).orderBy('prediction', ascending=True)\n",
        "\n",
        "# Show predictions\n",
        "if VISUALIZATION:\n",
        "  predictions.select('process_id', 'prediction').show()\n",
        "  grouped_predictions.show(truncate=False)"
      ],
      "metadata": {
        "id": "JVyzfIj_Dg1u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if VISUALIZATION:\n",
        "  # Extract the PCA result and cluster assignments\n",
        "  pca_features = predictions.select('pca_features').rdd.map(lambda row: row[0]).collect()\n",
        "  pca_features = np.array(pca_features)\n",
        "  cluster_assignments = predictions.select('prediction').rdd.map(lambda row: row[0]).collect()\n",
        "\n",
        "  # Plot the PCA result in 2D\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  clusters = np.unique(cluster_assignments)\n",
        "  for cluster in clusters:\n",
        "      cluster_indices = np.where(np.array(cluster_assignments) == cluster)\n",
        "      plt.scatter(pca_features[cluster_indices, 0], pca_features[cluster_indices, 1], label=f'Cluster {cluster}')\n",
        "  plt.xlabel('PCA Component 1')\n",
        "  plt.ylabel('PCA Component 2')\n",
        "  plt.title('PCA of Clustered Data in 2D')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "igjqYTxaDgzi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Export part2Observations.txt"
      ],
      "metadata": {
        "id": "4ZUMhUBWcuzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"part2Observations.txt\", \"w\") as f:\n",
        "    grouped = []\n",
        "    for row in grouped_predictions.collect():\n",
        "        representative_id = row[\"prediction\"]\n",
        "        group_ids = sorted(row[\"process_ids\"])\n",
        "\n",
        "        f.write(f\"Group: {{{', '.join(map(str, group_ids))}}}\\n\")\n",
        "        for process_id in group_ids:\n",
        "            f.write(f\"{process_id}:\\n\")\n",
        "            # Filter logs directly for the current process_id\n",
        "            process_logs = logs_grouped_not_sorted.filter(col(\"process_id\") == process_id).collect()\n",
        "            for log in process_logs:\n",
        "                from_servers = log['from_servers']\n",
        "                to_servers = log['to_servers']\n",
        "                times = log['times']\n",
        "                actions = log['actions']\n",
        "                process_id_val = log['process_id']\n",
        "\n",
        "                # Iterate through the lists and create log entries\n",
        "                for from_server, to_server, time, action in zip(from_servers, to_servers, times, actions):\n",
        "                    log_str = f\"<{from_server}, {to_server}, {time}, {action}, {process_id_val}>\"\n",
        "                    f.write(f\"\\t\\t\\t{log_str}\\n\")\n",
        "\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "A_A6xNaPGPpl"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}