{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, regexp_replace, posexplode, struct, collect_list, posexplode_outer, hash as spark_hash, concat_ws, when, abs\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, LongType\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-22\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Spark/spark-3.5.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/Hadoop\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:/Users/chris/AppData/Local/Programs/Python/Python311/python.exe\" \n",
    "os.environ[\"PATH\"] = (\n",
    "    os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"SPARK_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"PYSPARK_PYTHON\"]) + os.pathsep +\n",
    "    os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Configure Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    # create the session\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"DIS-lab-1\")    # Sets name of the Spark Application\n",
    "    conf.setMaster(\"local[*]\")    # Master URL. In this case local[*] uses all the available cores in the machine\n",
    "    conf.set(\"spark.driver.memory\", \"2G\")   # Memory allocated to driver process\n",
    "    conf.set(\"spark.driver.maxResultSize\", \"2G\")    # Maximum size of results that can be returned to driver\n",
    "    conf.set(\"spark.executor.memory\", \"1G\")    # Memory allocated to each executor     \n",
    "    sc = pyspark.SparkContext(conf=conf)    # Initializes tha Spark context with this specific configuration\n",
    "    spark = SparkSession.builder.getOrCreate()    # Creates Spark session\n",
    "    \n",
    "    return sc, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Stopped existing SparkContext\n",
      "--Stopped existing SparkSession\n",
      "Spark session created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Belmont.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DIS-lab-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x229a11f4810>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    if 'sc' in globals() and sc is not None:\n",
    "        sc.stop()\n",
    "        print(\"--Stopped existing SparkContext\")\n",
    "    if 'spark' in globals() and isinstance(spark, SparkSession):\n",
    "        spark.stop()\n",
    "        print(\"--Stopped existing SparkSession\")\n",
    "except Exception as e:\n",
    "    print(f\"Error stopping existing Spark session or context: {e}\")\n",
    "\n",
    "# Create a new Spark session\n",
    "sc, spark = create_session()\n",
    "print(\"Spark session created successfully!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Logs|\n",
      "+--------------------+\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = spark.read.csv('datasets/dataset_1.csv', header=True, inferSchema=True)\n",
    "logs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into 5 separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----+-------+----------+\n",
      "|from_server|   to_server|time| action|process_id|\n",
      "+-----------+------------+----+-------+----------+\n",
      "|       user| ui_server_2|   0|Request|      7127|\n",
      "|       user|ui_server_14|   0|Request|      6463|\n",
      "|       user|ui_server_14|   0|Request|      8002|\n",
      "|       user|ui_server_13|   0|Request|      6557|\n",
      "|       user|ui_server_10|   0|Request|      8193|\n",
      "|       user| ui_server_8|   0|Request|      4888|\n",
      "|       user| ui_server_8|   0|Request|      1412|\n",
      "|       user| ui_server_5|   0|Request|      3842|\n",
      "|       user|ui_server_16|   1|Request|      3161|\n",
      "|       user|ui_server_10|   1|Request|      5707|\n",
      "|       user| ui_server_5|   1|Request|      8909|\n",
      "|       user| ui_server_8|   1|Request|        53|\n",
      "|       user| ui_server_2|   1|Request|      9748|\n",
      "|       user| ui_server_4|   1|Request|      3333|\n",
      "|       user| ui_server_8|   1|Request|      6116|\n",
      "|       user|ui_server_17|   2|Request|      5921|\n",
      "|       user|ui_server_11|   2|Request|      1671|\n",
      "|       user| ui_server_2|   2|Request|      5744|\n",
      "|       user|ui_server_12|   2|Request|       446|\n",
      "|       user| ui_server_4|   2|Request|      8012|\n",
      "+-----------+------------+----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_splitted = logs \\\n",
    "    .withColumn(\"from_server\", regexp_replace(split(col(\"Logs\"), \", \").getItem(0), \"[<>]\", \"\")) \\\n",
    "    .withColumn(\"to_server\", split(col(\"Logs\"), \", \").getItem(1)) \\\n",
    "    .withColumn(\"time\", split(col(\"Logs\"), \", \").getItem(2)) \\\n",
    "    .withColumn(\"action\", split(col(\"Logs\"), \", \").getItem(3)) \\\n",
    "    .withColumn(\"process_id\", regexp_replace(split(col(\"Logs\"), \", \").getItem(4), \"[<>]\", \"\")) \\\n",
    "    .drop(\"Logs\")\n",
    "\n",
    "# Cast the \"time\" and \"process_id\" columns to integers\n",
    "logs_casted = logs_splitted \\\n",
    "    .withColumn(\"time\", col(\"time\").cast(\"integer\")) \\\n",
    "    .withColumn(\"process_id\", col(\"process_id\").cast(\"integer\"))\n",
    "\n",
    "logs_casted.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by process_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|process_id|        from_servers|          to_servers|               times|             actions|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        28|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|\n",
      "|        31|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|\n",
      "|        34|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|\n",
      "|        53|[limit_check_amer...|[american_express...|[358, 362, 370, 3...|[Response, Respon...|\n",
      "|        65|[inventory_update...|[seasonal_adjustm...|[375, 404, 428, 4...|[Request, Request...|\n",
      "|        78|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|\n",
      "|        81|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|\n",
      "|        85|[review_server_4,...|[review_verificat...|[360, 369, 382, 3...|[Request, Request...|\n",
      "|       101|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|\n",
      "|       108|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|\n",
      "|       115|[user, ui_server_...|[ui_server_18, pu...|[265, 270, 278, 2...|[Request, Request...|\n",
      "|       126|[user, ui_server_...|[ui_server_16, pu...|[841, 848, 858, 8...|[Request, Request...|\n",
      "|       133|[limit_check_visa...|[currency_convers...|[357, 359, 364, 3...|[Request, Request...|\n",
      "|       137|[user, ui_server_...|[ui_server_8, pur...|[578, 581, 586, 5...|[Request, Request...|\n",
      "|       148|[ad_server_7, aut...|[automatic_respon...|[360, 368, 380, 4...|[Response, Respon...|\n",
      "|       155|[user, ui_server_...|[ui_server_3, pur...|[923, 924, 932, 9...|[Request, Request...|\n",
      "|       183|[user, ui_server_...|[ui_server_10, pu...|[514, 518, 525, 5...|[Request, Request...|\n",
      "|       193|[user, ui_server_...|[ui_server_5, pur...|[224, 228, 238, 2...|[Request, Request...|\n",
      "|       210|[user, ui_server_...|[ui_server_10, pu...|[233, 236, 243, 2...|[Request, Request...|\n",
      "|       211|[user, ui_server_...|[ui_server_17, pu...|[460, 465, 473, 4...|[Request, Request...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_grouped = logs_casted.groupBy(\"process_id\").agg(\n",
    "    collect_list(\"from_server\").alias(\"from_servers\"),\n",
    "    collect_list(\"to_server\").alias(\"to_servers\"),\n",
    "    collect_list(\"time\").alias(\"times\"),\n",
    "    collect_list(\"action\").alias(\"actions\")\n",
    ")\n",
    "logs_grouped.show(20, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort all the columns based on times so the sub-processes are in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lists(times, from_servers, to_servers, actions):\n",
    "    combined = list(zip(times, from_servers, to_servers, actions))\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0])\n",
    "    times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted = zip(*sorted_combined)\n",
    "    return list(times_sorted), list(from_servers_sorted), list(to_servers_sorted), list(actions_sorted)\n",
    "\n",
    "# Define the schema for the sorted columns\n",
    "sorted_lists_schema = StructType([\n",
    "    StructField(\"times\", ArrayType(LongType()), nullable=True),\n",
    "    StructField(\"from_servers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"to_servers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"actions\", ArrayType(StringType()), nullable=True)\n",
    "])\n",
    "\n",
    "# Register the function as a UDF\n",
    "sort_lists_udf = udf(sort_lists, sorted_lists_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|        from_servers|          to_servers|               times|             actions|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[1, 8, 18, 22, 32...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[253, 256, 266, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_14, pu...|[166, 172, 181, 1...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_18, pu...|[265, 270, 278, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_16, pu...|[841, 848, 858, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_11, pu...|[256, 261, 267, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[578, 581, 586, 5...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_20, pu...|[82, 93, 100, 112...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_3, pur...|[923, 924, 932, 9...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_10, pu...|[514, 518, 525, 5...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_5, pur...|[224, 228, 238, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_10, pu...|[233, 236, 243, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_17, pu...|[460, 465, 473, 4...|[Request, Request...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the UDF to sort the lists based on the \"times\" column\n",
    "logs_grouped = logs_grouped.withColumn(\"sorted_lists\", sort_lists_udf(\"times\", \"from_servers\", \"to_servers\", \"actions\"))\n",
    "\n",
    "# Split the sorted lists into separate columns\n",
    "logs_grouped = logs_grouped.withColumn(\"times\", col(\"sorted_lists.times\")) \\\n",
    "                           .withColumn(\"from_servers\", col(\"sorted_lists.from_servers\")) \\\n",
    "                           .withColumn(\"to_servers\", col(\"sorted_lists.to_servers\")) \\\n",
    "                           .withColumn(\"actions\", col(\"sorted_lists.actions\")) \\\n",
    "                           .drop(\"sorted_lists\")\n",
    "\n",
    "# Show the sorted logs\n",
    "logs_grouped.select(\"from_servers\", \"to_servers\", \"times\", \"actions\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the from_servers sequence as feature for hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------+\n",
      "|process_id|        from_servers|          to_servers|               times|             actions|      process_string|process_hash|bucket|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------+\n",
      "|        28|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|user,ui_server_1,...|   189349086|    86|\n",
      "|        31|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|user,ui_server_6,...|  1025501846|    46|\n",
      "|        34|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|user,ui_server_19...|   932983399|   199|\n",
      "|        53|[user, ui_server_...|[ui_server_8, pur...|[1, 8, 18, 22, 32...|[Request, Request...|user,ui_server_8,...|  1849055925|   125|\n",
      "|        65|[user, ui_server_...|[ui_server_8, pur...|[253, 256, 266, 2...|[Request, Request...|user,ui_server_8,...|  1160153491|    91|\n",
      "|        78|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|user,ui_server_12...|   408352463|    63|\n",
      "|        81|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|user,ui_server_3,...|  2018519219|    19|\n",
      "|        85|[user, ui_server_...|[ui_server_14, pu...|[166, 172, 181, 1...|[Request, Request...|user,ui_server_14...|  1932070369|   169|\n",
      "|       101|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|user,ui_server_12...|  1679723677|    77|\n",
      "|       108|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|user,ui_server_13...|   771513310|   110|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Using the withColumn method to create a new column \"process_string\".\n",
    "The concat_ws function concatenates multiple column values into a single string, separated by a comma.\n",
    "col(\"from_servers\"): Selects the column \"from_servers\".\n",
    "col(\"to_servers\"): Selects the column \"to_servers\".\n",
    "col(\"times\"): Selects the column \"times\".\n",
    "col(\"actions\"): Selects the column \"actions\".\n",
    "The concatenated string is stored in the new column \"process_string\".\n",
    "'''\n",
    "\n",
    "logs_grouped = logs_grouped.withColumn(\n",
    "    \"process_string\",\n",
    "    concat_ws(\n",
    "        \",\",\n",
    "        col(\"from_servers\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "logs_grouped.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine number of buckets based on Sturges' formula and hash based on process_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num buckets: 200\n",
      "+----------+--------------------+------------+------+\n",
      "|process_id|      process_string|process_hash|bucket|\n",
      "+----------+--------------------+------------+------+\n",
      "|        28|user,ui_server_1,...|   189349086|    86|\n",
      "|        31|user,ui_server_6,...|  1025501846|    46|\n",
      "|        34|user,ui_server_19...|   932983399|   199|\n",
      "|        53|user,ui_server_8,...|  1849055925|   125|\n",
      "|        65|user,ui_server_8,...|  1160153491|    91|\n",
      "|        78|user,ui_server_12...|   408352463|    63|\n",
      "|        81|user,ui_server_3,...|  2018519219|    19|\n",
      "|        85|user,ui_server_14...|  1932070369|   169|\n",
      "|       101|user,ui_server_12...|  1679723677|    77|\n",
      "|       108|user,ui_server_13...|   771513310|   110|\n",
      "+----------+--------------------+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute a hash value for each process string\n",
    "logs_grouped = logs_grouped.withColumn(\"process_hash\", abs(spark_hash(col(\"process_string\"))))\n",
    "# withColumn is used to create a new column \"process_hash\" by applying the spark_hash function to the \"process_string\" column.\n",
    "\n",
    "# Calculate the number of processes\n",
    "num_processes = logs_grouped.count()\n",
    "\n",
    "# Determine the number of buckets using Sturges' formula\n",
    "num_buckets = int((num_processes ** 0.5) * 2)\n",
    "print(f\"Num buckets: {num_buckets}\")\n",
    "\n",
    "# Assign each process to a bucket\n",
    "logs_grouped = logs_grouped.withColumn(\"bucket\", (col(\"process_hash\") % num_buckets).cast(\"int\"))\n",
    "# withColumn is used to create a new column \"bucket\".\n",
    "# The process hash value is divided by the number of buckets and the remainder is taken (modulus operation).\n",
    "# The result is cast to an integer, assigning each process to one of the buckets.\n",
    "\n",
    "# Show the grouped logs with bucket assignments\n",
    "logs_grouped.select(\"process_id\", \"process_string\", \"process_hash\", \"bucket\").show(10, truncate=True)\n",
    "# Display the columns \"process_id\", \"process_string\", \"process_hash\", and \"bucket\" \n",
    "\n",
    "# Group processes into buckets and collect results\n",
    "bucketed_processes = logs_grouped.groupBy(\"bucket\").agg(collect_list(\"process_id\").alias(\"process_ids\"))\n",
    "# Group the logs by \"bucket\" and collect the \"process_id\"s of the logs in each bucket.\n",
    "# The collected \"process_id\"s are stored in a new column named \"process_ids\".\n",
    "\n",
    "# Show the bucketed processes\n",
    "bucketed_processes.show(25, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
