{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and environment initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, regexp_replace, posexplode, struct, collect_list, posexplode_outer, hash as spark_hash, concat_ws, when, abs\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, LongType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-22\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Spark/spark-3.5.1-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/Hadoop\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:/Users/chris/AppData/Local/Programs/Python/Python311/python.exe\" \n",
    "os.environ[\"PATH\"] = (\n",
    "    os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"SPARK_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\") + os.pathsep +\n",
    "    os.path.join(os.environ[\"PYSPARK_PYTHON\"]) + os.pathsep +\n",
    "    os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & Configure Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    # create the session\n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"DIS-lab-1\")    # Sets name of the Spark Application\n",
    "    conf.setMaster(\"local[*]\")    # Master URL. In this case local[*] uses all the available cores in the machine\n",
    "    conf.set(\"spark.driver.memory\", \"2G\")   # Memory allocated to driver process\n",
    "    conf.set(\"spark.driver.maxResultSize\", \"2G\")    # Maximum size of results that can be returned to driver\n",
    "    conf.set(\"spark.executor.memory\", \"1G\")    # Memory allocated to each executor     \n",
    "    sc = pyspark.SparkContext(conf=conf)    # Initializes tha Spark context with this specific configuration\n",
    "    spark = SparkSession.builder.getOrCreate()    # Creates Spark session\n",
    "    \n",
    "    return sc, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Stopped existing SparkContext\n",
      "--Stopped existing SparkSession\n",
      "Spark session created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Belmont.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DIS-lab-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x229a30b1650>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    if 'sc' in globals() and sc is not None:\n",
    "        sc.stop()\n",
    "        print(\"--Stopped existing SparkContext\")\n",
    "    if 'spark' in globals() and isinstance(spark, SparkSession):\n",
    "        spark.stop()\n",
    "        print(\"--Stopped existing SparkSession\")\n",
    "except Exception as e:\n",
    "    print(f\"Error stopping existing Spark session or context: {e}\")\n",
    "\n",
    "# Create a new Spark session\n",
    "sc, spark = create_session()\n",
    "print(\"Spark session created successfully!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Logs|\n",
      "+--------------------+\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "|<user, ui_server_...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs = spark.read.csv('datasets/dataset_1.csv', header=True, inferSchema=True)\n",
    "logs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into 5 separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----+-------+----------+\n",
      "|from_server|   to_server|time| action|process_id|\n",
      "+-----------+------------+----+-------+----------+\n",
      "|       user| ui_server_2|   0|Request|      7127|\n",
      "|       user|ui_server_14|   0|Request|      6463|\n",
      "|       user|ui_server_14|   0|Request|      8002|\n",
      "|       user|ui_server_13|   0|Request|      6557|\n",
      "|       user|ui_server_10|   0|Request|      8193|\n",
      "|       user| ui_server_8|   0|Request|      4888|\n",
      "|       user| ui_server_8|   0|Request|      1412|\n",
      "|       user| ui_server_5|   0|Request|      3842|\n",
      "|       user|ui_server_16|   1|Request|      3161|\n",
      "|       user|ui_server_10|   1|Request|      5707|\n",
      "|       user| ui_server_5|   1|Request|      8909|\n",
      "|       user| ui_server_8|   1|Request|        53|\n",
      "|       user| ui_server_2|   1|Request|      9748|\n",
      "|       user| ui_server_4|   1|Request|      3333|\n",
      "|       user| ui_server_8|   1|Request|      6116|\n",
      "|       user|ui_server_17|   2|Request|      5921|\n",
      "|       user|ui_server_11|   2|Request|      1671|\n",
      "|       user| ui_server_2|   2|Request|      5744|\n",
      "|       user|ui_server_12|   2|Request|       446|\n",
      "|       user| ui_server_4|   2|Request|      8012|\n",
      "+-----------+------------+----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_splitted = logs \\\n",
    "    .withColumn(\"from_server\", regexp_replace(split(col(\"Logs\"), \", \").getItem(0), \"[<>]\", \"\")) \\\n",
    "    .withColumn(\"to_server\", split(col(\"Logs\"), \", \").getItem(1)) \\\n",
    "    .withColumn(\"time\", split(col(\"Logs\"), \", \").getItem(2)) \\\n",
    "    .withColumn(\"action\", split(col(\"Logs\"), \", \").getItem(3)) \\\n",
    "    .withColumn(\"process_id\", regexp_replace(split(col(\"Logs\"), \", \").getItem(4), \"[<>]\", \"\")) \\\n",
    "    .drop(\"Logs\")\n",
    "\n",
    "# Cast the \"time\" and \"process_id\" columns to integers\n",
    "logs_casted = logs_splitted \\\n",
    "    .withColumn(\"time\", col(\"time\").cast(\"integer\")) \\\n",
    "    .withColumn(\"process_id\", col(\"process_id\").cast(\"integer\"))\n",
    "\n",
    "logs_casted.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by process_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|process_id|        from_servers|          to_servers|               times|             actions|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        28|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|\n",
      "|        31|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|\n",
      "|        34|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|\n",
      "|        53|[limit_check_amer...|[american_express...|[358, 362, 370, 3...|[Response, Respon...|\n",
      "|        65|[inventory_update...|[seasonal_adjustm...|[375, 404, 428, 4...|[Request, Request...|\n",
      "|        78|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|\n",
      "|        81|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|\n",
      "|        85|[review_server_4,...|[review_verificat...|[360, 369, 382, 3...|[Request, Request...|\n",
      "|       101|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|\n",
      "|       108|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|\n",
      "|       115|[user, ui_server_...|[ui_server_18, pu...|[265, 270, 278, 2...|[Request, Request...|\n",
      "|       126|[user, ui_server_...|[ui_server_16, pu...|[841, 848, 858, 8...|[Request, Request...|\n",
      "|       133|[limit_check_visa...|[currency_convers...|[357, 359, 364, 3...|[Request, Request...|\n",
      "|       137|[user, ui_server_...|[ui_server_8, pur...|[578, 581, 586, 5...|[Request, Request...|\n",
      "|       148|[ad_server_7, aut...|[automatic_respon...|[360, 368, 380, 4...|[Response, Respon...|\n",
      "|       155|[user, ui_server_...|[ui_server_3, pur...|[923, 924, 932, 9...|[Request, Request...|\n",
      "|       183|[user, ui_server_...|[ui_server_10, pu...|[514, 518, 525, 5...|[Request, Request...|\n",
      "|       193|[user, ui_server_...|[ui_server_5, pur...|[224, 228, 238, 2...|[Request, Request...|\n",
      "|       210|[user, ui_server_...|[ui_server_10, pu...|[233, 236, 243, 2...|[Request, Request...|\n",
      "|       211|[user, ui_server_...|[ui_server_17, pu...|[460, 465, 473, 4...|[Request, Request...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_grouped = logs_casted.groupBy(\"process_id\").agg(\n",
    "    collect_list(\"from_server\").alias(\"from_servers\"),\n",
    "    collect_list(\"to_server\").alias(\"to_servers\"),\n",
    "    collect_list(\"time\").alias(\"times\"),\n",
    "    collect_list(\"action\").alias(\"actions\")\n",
    ")\n",
    "logs_grouped.show(20, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort all the columns based on times so the sub-processes are in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_lists(times, from_servers, to_servers, actions):\n",
    "    combined = list(zip(times, from_servers, to_servers, actions))\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0])\n",
    "    times_sorted, from_servers_sorted, to_servers_sorted, actions_sorted = zip(*sorted_combined)\n",
    "    return list(times_sorted), list(from_servers_sorted), list(to_servers_sorted), list(actions_sorted)\n",
    "\n",
    "# Define the schema for the sorted columns\n",
    "sorted_lists_schema = StructType([\n",
    "    StructField(\"times\", ArrayType(LongType()), nullable=True),\n",
    "    StructField(\"from_servers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"to_servers\", ArrayType(StringType()), nullable=True),\n",
    "    StructField(\"actions\", ArrayType(StringType()), nullable=True)\n",
    "])\n",
    "\n",
    "# Register the function as a UDF\n",
    "sort_lists_udf = udf(sort_lists, sorted_lists_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|        from_servers|          to_servers|               times|             actions|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[1, 8, 18, 22, 32...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[253, 256, 266, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_14, pu...|[166, 172, 181, 1...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_18, pu...|[265, 270, 278, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_16, pu...|[841, 848, 858, 8...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_11, pu...|[256, 261, 267, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_8, pur...|[578, 581, 586, 5...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_20, pu...|[82, 93, 100, 112...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_3, pur...|[923, 924, 932, 9...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_10, pu...|[514, 518, 525, 5...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_5, pur...|[224, 228, 238, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_10, pu...|[233, 236, 243, 2...|[Request, Request...|\n",
      "|[user, ui_server_...|[ui_server_17, pu...|[460, 465, 473, 4...|[Request, Request...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the UDF to sort the lists based on the \"times\" column\n",
    "logs_grouped = logs_grouped.withColumn(\"sorted_lists\", sort_lists_udf(\"times\", \"from_servers\", \"to_servers\", \"actions\"))\n",
    "\n",
    "# Split the sorted lists into separate columns\n",
    "logs_grouped = logs_grouped.withColumn(\"times\", col(\"sorted_lists.times\")) \\\n",
    "                           .withColumn(\"from_servers\", col(\"sorted_lists.from_servers\")) \\\n",
    "                           .withColumn(\"to_servers\", col(\"sorted_lists.to_servers\")) \\\n",
    "                           .withColumn(\"actions\", col(\"sorted_lists.actions\")) \\\n",
    "                           .drop(\"sorted_lists\")\n",
    "\n",
    "# Show the sorted logs\n",
    "logs_grouped.select(\"from_servers\", \"to_servers\", \"times\", \"actions\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the from_servers sequence as feature for hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|process_id|        from_servers|          to_servers|               times|             actions|      process_string|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        28|[user, ui_server_...|[ui_server_1, pur...|[771, 774, 782, 7...|[Request, Request...|user,ui_server_1,...|\n",
      "|        31|[user, ui_server_...|[ui_server_6, pur...|[972, 974, 981, 1...|[Request, Request...|user,ui_server_6,...|\n",
      "|        34|[user, ui_server_...|[ui_server_19, pu...|[856, 867, 874, 8...|[Request, Request...|user,ui_server_19...|\n",
      "|        53|[user, ui_server_...|[ui_server_8, pur...|[1, 8, 18, 22, 32...|[Request, Request...|user,ui_server_8,...|\n",
      "|        65|[user, ui_server_...|[ui_server_8, pur...|[253, 256, 266, 2...|[Request, Request...|user,ui_server_8,...|\n",
      "|        78|[user, ui_server_...|[ui_server_12, pu...|[381, 385, 390, 3...|[Request, Request...|user,ui_server_12...|\n",
      "|        81|[user, ui_server_...|[ui_server_3, pur...|[797, 800, 809, 8...|[Request, Request...|user,ui_server_3,...|\n",
      "|        85|[user, ui_server_...|[ui_server_14, pu...|[166, 172, 181, 1...|[Request, Request...|user,ui_server_14...|\n",
      "|       101|[user, ui_server_...|[ui_server_12, pu...|[379, 385, 391, 4...|[Request, Request...|user,ui_server_12...|\n",
      "|       108|[user, ui_server_...|[ui_server_13, pu...|[79, 88, 93, 112,...|[Request, Request...|user,ui_server_13...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Using the withColumn method to create a new column \"process_string\".\n",
    "The concat_ws function concatenates multiple column values into a single string, separated by a comma.\n",
    "col(\"from_servers\"): Selects the column \"from_servers\".\n",
    "col(\"to_servers\"): Selects the column \"to_servers\".\n",
    "col(\"times\"): Selects the column \"times\".\n",
    "col(\"actions\"): Selects the column \"actions\".\n",
    "The concatenated string is stored in the new column \"process_string\".\n",
    "'''\n",
    "\n",
    "logs_grouped = logs_grouped.withColumn(\n",
    "    \"process_string\",\n",
    "    concat_ws(\n",
    "        \",\",\n",
    "        col(\"from_servers\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "logs_grouped.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine number of buckets based on Sturges' formula and hash based on process_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num buckets: 200\n",
      "+----------+--------------------+------------+------+\n",
      "|process_id|      process_string|process_hash|bucket|\n",
      "+----------+--------------------+------------+------+\n",
      "|        28|user,ui_server_1,...|   189349086|    86|\n",
      "|        31|user,ui_server_6,...|  1025501846|    46|\n",
      "|        34|user,ui_server_19...|   932983399|   199|\n",
      "|        53|user,ui_server_8,...|  1849055925|   125|\n",
      "|        65|user,ui_server_8,...|  1160153491|    91|\n",
      "|        78|user,ui_server_12...|   408352463|    63|\n",
      "|        81|user,ui_server_3,...|  2018519219|    19|\n",
      "|        85|user,ui_server_14...|  1932070369|   169|\n",
      "|       101|user,ui_server_12...|  1679723677|    77|\n",
      "|       108|user,ui_server_13...|   771513310|   110|\n",
      "+----------+--------------------+------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+--------------------+\n",
      "|bucket|         process_ids|\n",
      "+------+--------------------+\n",
      "|     0|[2797, 5926, 6741...|\n",
      "|     1|[2726, 9059, 9395...|\n",
      "|     2|[2659, 4683, 7649...|\n",
      "|     3|[3017, 5024, 6337...|\n",
      "|     4|[1019, 1398, 1886...|\n",
      "|     5|[5642, 5691, 5823...|\n",
      "|     6|[6391, 330, 4428,...|\n",
      "|     7|[4457, 6668, 7265...|\n",
      "|     8|[4874, 5160, 8130...|\n",
      "|     9|[3173, 4523, 8290...|\n",
      "|    10|[9096, 602, 3927,...|\n",
      "|    11|[2996, 9097, 7796...|\n",
      "|    12|[7424, 7928, 9674...|\n",
      "|    13|[3083, 3679, 4720...|\n",
      "|    14|[2513, 4452, 6361...|\n",
      "|    15|[5238, 8417, 4032...|\n",
      "|    16|[4354, 6045, 6863...|\n",
      "|    17|[3072, 3792, 7198...|\n",
      "|    18|[2655, 1344, 1537...|\n",
      "|    19|[2532, 3284, 7225...|\n",
      "|    20|[5562, 1294, 1291...|\n",
      "|    21|[2888, 4477, 5907...|\n",
      "|    22|[5006, 5037, 5586...|\n",
      "|    23|[2754, 3071, 4610...|\n",
      "|    24|[5886, 8247, 8260...|\n",
      "+------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute a hash value for each process string\n",
    "logs_grouped = logs_grouped.withColumn(\"process_hash\", abs(spark_hash(col(\"process_string\"))))\n",
    "# withColumn is used to create a new column \"process_hash\" by applying the spark_hash function to the \"process_string\" column.\n",
    "\n",
    "# Calculate the number of processes\n",
    "num_processes = logs_grouped.count()\n",
    "\n",
    "# Determine the number of buckets using Sturges' formula\n",
    "num_buckets = int((num_processes ** 0.5) * 2)\n",
    "print(f\"Num buckets: {num_buckets}\")\n",
    "\n",
    "# Assign each process to a bucket\n",
    "logs_grouped = logs_grouped.withColumn(\"bucket\", (col(\"process_hash\") % num_buckets).cast(\"int\"))\n",
    "# withColumn is used to create a new column \"bucket\".\n",
    "# The process hash value is divided by the number of buckets and the remainder is taken (modulus operation).\n",
    "# The result is cast to an integer, assigning each process to one of the buckets.\n",
    "\n",
    "# Show the grouped logs with bucket assignments\n",
    "logs_grouped.select(\"process_id\", \"process_string\", \"process_hash\", \"bucket\").show(10, truncate=True)\n",
    "# Display the columns \"process_id\", \"process_string\", \"process_hash\", and \"bucket\" \n",
    "\n",
    "# Group processes into buckets and collect results\n",
    "bucketed_processes = logs_grouped.groupBy(\"bucket\").agg(collect_list(\"process_id\").alias(\"process_ids\"))\n",
    "# Group the logs by \"bucket\" and collect the \"process_id\"s of the logs in each bucket.\n",
    "# The collected \"process_id\"s are stored in a new column named \"process_ids\".\n",
    "\n",
    "# Show the bucketed processes\n",
    "bucketed_processes.show(25, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marios Map-Reuce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine unique processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processes: 10000\n"
     ]
    }
   ],
   "source": [
    "# Collect and structure the data as key-value pairs\n",
    "unique_processes_rdd = logs_grouped.select(\"process_id\", \"times\", \"from_servers\", \"to_servers\", \"actions\").rdd.map(\n",
    "    lambda row: (row[\"process_id\"], {\n",
    "        \"times\": row[\"times\"],\n",
    "        \"from_servers\": row[\"from_servers\"],\n",
    "        \"to_servers\": row[\"to_servers\"],\n",
    "        \"actions\": row[\"actions\"]\n",
    "    })\n",
    ")\n",
    "\n",
    "# Convert to a dictionary and broadcast\n",
    "unique_processes = dict(unique_processes_rdd.collect())\n",
    "broadcast_processes = spark.sparkContext.broadcast(unique_processes)\n",
    "\n",
    "N = len(unique_processes)\n",
    "print(f'Number of processes: {N}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calculate_similarity function\n",
    "def calculate_similarity(times1, times2):\n",
    "    set1 = set(times1)\n",
    "    set2 = set(times2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union if union != 0 else 0\n",
    "    return similarity\n",
    "\n",
    "def mapper_function(times, from_servers, to_servers, actions):\n",
    "    similarities = []\n",
    "    for other_process_id, other_process in broadcast_processes.value.items():\n",
    "        other_times = other_process[\"times\"]\n",
    "        similarity = calculate_similarity(times, other_times)\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "mapper_udf = udf(lambda times, from_servers, to_servers, actions: mapper_function(times, from_servers, to_servers, actions), ArrayType(DoubleType()))\n",
    "\n",
    "logs_grouped = logs_grouped.withColumn(\"similarity_row\", mapper_udf(col(\"times\"), col(\"from_servers\"), col(\"to_servers\"), col(\"actions\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|process_id|matches|\n",
      "+----------+-------+\n",
      "|28        |[28]   |\n",
      "|31        |[31]   |\n",
      "|34        |[34]   |\n",
      "|53        |[53]   |\n",
      "|65        |[65]   |\n",
      "|78        |[78]   |\n",
      "|81        |[81]   |\n",
      "|85        |[85]   |\n",
      "|101       |[101]  |\n",
      "|108       |[108]  |\n",
      "|115       |[115]  |\n",
      "|126       |[126]  |\n",
      "|133       |[133]  |\n",
      "|137       |[137]  |\n",
      "|148       |[148]  |\n",
      "|155       |[155]  |\n",
      "|183       |[183]  |\n",
      "|193       |[193]  |\n",
      "|210       |[210]  |\n",
      "|211       |[211]  |\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect the similarity rows\n",
    "similarity_rows = logs_grouped.select(\"similarity_row\").collect()\n",
    "\n",
    "# Create the similarity matrix\n",
    "similarity_matrix = np.array([row.similarity_row for row in similarity_rows])\n",
    "\n",
    "# Identify matching processes based on the similarity threshold\n",
    "threshold = 0.5\n",
    "matches = []\n",
    "for i in range(len(unique_processes)):\n",
    "    match = [list(unique_processes.keys())[j] for j in range(len(unique_processes)) if similarity_matrix[i][j] > threshold]\n",
    "    matches.append(match)\n",
    "\n",
    "# Output the matches\n",
    "matches_df = spark.createDataFrame([(list(unique_processes.keys())[i], matches[i]) for i in range(len(unique_processes))], [\"process_id\", \"matches\"])\n",
    "matches_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|process_id|matches|\n",
      "+----------+-------+\n",
      "|28        |[28]   |\n",
      "|31        |[31]   |\n",
      "|34        |[34]   |\n",
      "|53        |[53]   |\n",
      "|65        |[65]   |\n",
      "|78        |[78]   |\n",
      "|81        |[81]   |\n",
      "|85        |[85]   |\n",
      "|101       |[101]  |\n",
      "|108       |[108]  |\n",
      "|115       |[115]  |\n",
      "|126       |[126]  |\n",
      "|133       |[133]  |\n",
      "|137       |[137]  |\n",
      "|148       |[148]  |\n",
      "|155       |[155]  |\n",
      "|183       |[183]  |\n",
      "|193       |[193]  |\n",
      "|210       |[210]  |\n",
      "|211       |[211]  |\n",
      "|243       |[243]  |\n",
      "|251       |[251]  |\n",
      "|255       |[255]  |\n",
      "|296       |[296]  |\n",
      "|321       |[321]  |\n",
      "|322       |[322]  |\n",
      "|362       |[362]  |\n",
      "|368       |[368]  |\n",
      "|375       |[375]  |\n",
      "|385       |[385]  |\n",
      "|392       |[392]  |\n",
      "|436       |[436]  |\n",
      "|451       |[451]  |\n",
      "|458       |[458]  |\n",
      "|463       |[463]  |\n",
      "|471       |[471]  |\n",
      "|472       |[472]  |\n",
      "|481       |[481]  |\n",
      "|496       |[496]  |\n",
      "|497       |[497]  |\n",
      "|513       |[513]  |\n",
      "|516       |[516]  |\n",
      "|530       |[530]  |\n",
      "|540       |[540]  |\n",
      "|580       |[580]  |\n",
      "|588       |[588]  |\n",
      "|593       |[593]  |\n",
      "|596       |[596]  |\n",
      "|597       |[597]  |\n",
      "|613       |[613]  |\n",
      "+----------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_df.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
